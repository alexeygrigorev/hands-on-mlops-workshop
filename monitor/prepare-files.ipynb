{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12eb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a588458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = os.getenv('MODEL_VERSION')\n",
    "MODEL_URI = os.getenv('MODEL_URI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e9ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = '7c373fc9626549ed91cebb714b07e60a'\n",
    "MODEL_URI = 's3://mlflow-models-alexey/1/7c373fc9626549ed91cebb714b07e60a/artifacts/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094a4528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/18 21:19:46 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.4.1, required: mlflow==2.4)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(MODEL_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7efda0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PULocationID'] = ride['PULocationID']\n",
    "    features['DOLocationID'] = ride['DOLocationID']\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "\n",
    "\n",
    "def predict(features):\n",
    "    preds = model.predict(features)\n",
    "    return float(preds[0])\n",
    "\n",
    "\n",
    "def predict_endpoint(body):\n",
    "    ride = body['ride']\n",
    "    ride_id = body['ride_id']\n",
    "\n",
    "    features = prepare_features(ride)\n",
    "    pred = predict(features)\n",
    "\n",
    "    result = {\n",
    "        'prediction': {\n",
    "            'duration': pred,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    prediction_event = {\n",
    "        'ride_id': ride_id,\n",
    "        'ride': ride,\n",
    "        'features': features,\n",
    "        'prediction': result,\n",
    "        'version': MODEL_VERSION,\n",
    "    }\n",
    "\n",
    "    return prediction_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('data')\n",
    "data.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5015a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dc0244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'ride_id',\n",
    "    'lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
    "    'PULocationID', 'DOLocationID',\n",
    "    'trip_distance', 'duration'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88b91b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 2022 1...\n",
      "saved data/2022/01/2022-01-full.parquet\n",
      "saved data/2022/01/2022-01-01.parquet\n",
      "saved data/2022/01/2022-01-02.parquet\n",
      "saved data/2022/01/2022-01-03.parquet\n",
      "saved data/2022/01/2022-01-04.parquet\n",
      "saved data/2022/01/2022-01-05.parquet\n",
      "saved data/2022/01/2022-01-06.parquet\n",
      "saved data/2022/01/2022-01-07.parquet\n",
      "saved data/2022/01/2022-01-08.parquet\n",
      "saved data/2022/01/2022-01-09.parquet\n",
      "saved data/2022/01/2022-01-10.parquet\n",
      "saved data/2022/01/2022-01-11.parquet\n",
      "saved data/2022/01/2022-01-12.parquet\n",
      "saved data/2022/01/2022-01-13.parquet\n",
      "saved data/2022/01/2022-01-14.parquet\n",
      "saved data/2022/01/2022-01-15.parquet\n",
      "saved data/2022/01/2022-01-16.parquet\n",
      "saved data/2022/01/2022-01-17.parquet\n",
      "saved data/2022/01/2022-01-18.parquet\n",
      "saved data/2022/01/2022-01-19.parquet\n",
      "saved data/2022/01/2022-01-20.parquet\n",
      "saved data/2022/01/2022-01-21.parquet\n",
      "saved data/2022/01/2022-01-22.parquet\n",
      "saved data/2022/01/2022-01-23.parquet\n",
      "saved data/2022/01/2022-01-24.parquet\n",
      "saved data/2022/01/2022-01-25.parquet\n",
      "saved data/2022/01/2022-01-26.parquet\n",
      "saved data/2022/01/2022-01-27.parquet\n",
      "saved data/2022/01/2022-01-28.parquet\n",
      "saved data/2022/01/2022-01-29.parquet\n",
      "saved data/2022/01/2022-01-30.parquet\n",
      "saved data/2022/01/2022-01-31.parquet\n",
      "\n",
      "processing 2022 2...\n",
      "saved data/2022/02/2022-02-full.parquet\n",
      "saved data/2022/02/2022-02-01.parquet\n",
      "saved data/2022/02/2022-02-02.parquet\n",
      "saved data/2022/02/2022-02-03.parquet\n",
      "saved data/2022/02/2022-02-04.parquet\n",
      "saved data/2022/02/2022-02-05.parquet\n",
      "saved data/2022/02/2022-02-06.parquet\n",
      "saved data/2022/02/2022-02-07.parquet\n",
      "saved data/2022/02/2022-02-08.parquet\n",
      "saved data/2022/02/2022-02-09.parquet\n",
      "saved data/2022/02/2022-02-10.parquet\n",
      "saved data/2022/02/2022-02-11.parquet\n",
      "saved data/2022/02/2022-02-12.parquet\n",
      "saved data/2022/02/2022-02-13.parquet\n",
      "saved data/2022/02/2022-02-14.parquet\n",
      "saved data/2022/02/2022-02-15.parquet\n",
      "saved data/2022/02/2022-02-16.parquet\n",
      "saved data/2022/02/2022-02-17.parquet\n",
      "saved data/2022/02/2022-02-18.parquet\n",
      "saved data/2022/02/2022-02-19.parquet\n",
      "saved data/2022/02/2022-02-20.parquet\n",
      "saved data/2022/02/2022-02-21.parquet\n",
      "saved data/2022/02/2022-02-22.parquet\n",
      "saved data/2022/02/2022-02-23.parquet\n",
      "saved data/2022/02/2022-02-24.parquet\n",
      "saved data/2022/02/2022-02-25.parquet\n",
      "saved data/2022/02/2022-02-26.parquet\n",
      "saved data/2022/02/2022-02-27.parquet\n",
      "saved data/2022/02/2022-02-28.parquet\n",
      "\n",
      "processing 2022 3...\n",
      "saved data/2022/03/2022-03-full.parquet\n",
      "saved data/2022/03/2022-03-01.parquet\n",
      "saved data/2022/03/2022-03-02.parquet\n",
      "saved data/2022/03/2022-03-03.parquet\n",
      "saved data/2022/03/2022-03-04.parquet\n",
      "saved data/2022/03/2022-03-05.parquet\n",
      "saved data/2022/03/2022-03-06.parquet\n",
      "saved data/2022/03/2022-03-07.parquet\n",
      "saved data/2022/03/2022-03-08.parquet\n",
      "saved data/2022/03/2022-03-09.parquet\n",
      "saved data/2022/03/2022-03-10.parquet\n",
      "saved data/2022/03/2022-03-11.parquet\n",
      "saved data/2022/03/2022-03-12.parquet\n",
      "saved data/2022/03/2022-03-13.parquet\n",
      "saved data/2022/03/2022-03-14.parquet\n",
      "saved data/2022/03/2022-03-15.parquet\n",
      "saved data/2022/03/2022-03-16.parquet\n",
      "saved data/2022/03/2022-03-17.parquet\n",
      "saved data/2022/03/2022-03-18.parquet\n",
      "saved data/2022/03/2022-03-19.parquet\n",
      "saved data/2022/03/2022-03-20.parquet\n",
      "saved data/2022/03/2022-03-21.parquet\n",
      "saved data/2022/03/2022-03-22.parquet\n",
      "saved data/2022/03/2022-03-23.parquet\n",
      "saved data/2022/03/2022-03-24.parquet\n",
      "saved data/2022/03/2022-03-25.parquet\n",
      "saved data/2022/03/2022-03-26.parquet\n",
      "saved data/2022/03/2022-03-27.parquet\n",
      "saved data/2022/03/2022-03-28.parquet\n",
      "saved data/2022/03/2022-03-29.parquet\n",
      "saved data/2022/03/2022-03-30.parquet\n",
      "saved data/2022/03/2022-03-31.parquet\n",
      "\n",
      "processing 2023 1...\n",
      "saved data/2023/01/2023-01-full.parquet\n",
      "saved data/2023/01/2023-01-01.parquet\n",
      "saved data/2023/01/2023-01-02.parquet\n",
      "saved data/2023/01/2023-01-03.parquet\n",
      "saved data/2023/01/2023-01-04.parquet\n",
      "saved data/2023/01/2023-01-05.parquet\n",
      "saved data/2023/01/2023-01-06.parquet\n",
      "saved data/2023/01/2023-01-07.parquet\n",
      "saved data/2023/01/2023-01-08.parquet\n",
      "saved data/2023/01/2023-01-09.parquet\n",
      "saved data/2023/01/2023-01-10.parquet\n",
      "saved data/2023/01/2023-01-11.parquet\n",
      "saved data/2023/01/2023-01-12.parquet\n",
      "saved data/2023/01/2023-01-13.parquet\n",
      "saved data/2023/01/2023-01-14.parquet\n",
      "saved data/2023/01/2023-01-15.parquet\n",
      "saved data/2023/01/2023-01-16.parquet\n",
      "saved data/2023/01/2023-01-17.parquet\n",
      "saved data/2023/01/2023-01-18.parquet\n",
      "saved data/2023/01/2023-01-19.parquet\n",
      "saved data/2023/01/2023-01-20.parquet\n",
      "saved data/2023/01/2023-01-21.parquet\n",
      "saved data/2023/01/2023-01-22.parquet\n",
      "saved data/2023/01/2023-01-23.parquet\n",
      "saved data/2023/01/2023-01-24.parquet\n",
      "saved data/2023/01/2023-01-25.parquet\n",
      "saved data/2023/01/2023-01-26.parquet\n",
      "saved data/2023/01/2023-01-27.parquet\n",
      "saved data/2023/01/2023-01-28.parquet\n",
      "saved data/2023/01/2023-01-29.parquet\n",
      "saved data/2023/01/2023-01-30.parquet\n",
      "saved data/2023/01/2023-01-31.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_tuples = [(2022, 1), (2022, 2), (2022, 3), (2023, 1)]\n",
    "\n",
    "for y, m in data_tuples:\n",
    "    print(f'processing {y} {m}...')\n",
    "\n",
    "    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{y:04d}-{m:02d}.parquet'\n",
    "    df = read_dataframe(url)\n",
    "\n",
    "    df['ride_id'] = f'{y:04d}-{m:02d}-' + df.index.astype('str')\n",
    "    df = df[features]\n",
    "\n",
    "    rows = df[['PULocationID', 'DOLocationID', 'trip_distance']].to_dict(orient='records')\n",
    "    df['prediction'] = model.predict(rows)\n",
    "    \n",
    "    target_folder = data / f'{y:04d}' / f'{m:02d}' \n",
    "    target_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    target_file = target_folder / f'{y:04d}-{m:02d}-full.parquet'\n",
    "    df.to_parquet(target_file, index=False)\n",
    "    print(f'saved {target_file}')\n",
    "\n",
    "    first_day = datetime(year=y, month=m, day=1, hour=0, minute=0, second=0)\n",
    "    end_of_month = first_day + relativedelta(months=1)\n",
    "\n",
    "    today_midnight = first_day\n",
    "\n",
    "    while today_midnight < end_of_month:\n",
    "        tomorrow_midnight = today_midnight + timedelta(days=1)\n",
    "        \n",
    "        # chunk the original data\n",
    "\n",
    "        target_file = target_folder / f'{today_midnight.year:04d}-{today_midnight.month:02d}-{today_midnight.day:02d}.parquet'\n",
    "\n",
    "        df_today = df[(df.lpep_dropoff_datetime >= today_midnight) & (df.lpep_dropoff_datetime < tomorrow_midnight)]\n",
    "        df_today.to_parquet(target_file, index=False)\n",
    "\n",
    "        print(f'saved {target_file}')\n",
    "\n",
    "        # simulate predictions\n",
    "    \n",
    "        rows = df_today.to_dict(orient='records')\n",
    "\n",
    "        filename = f'{today_midnight.year:04d}-{today_midnight.month:02d}-{today_midnight.day:02d}-predictions.jsonl'\n",
    "        target_file = target_folder / filename\n",
    "\n",
    "        with target_file.open('wt', encoding='utf-8') as f_out:\n",
    "            for row in rows:\n",
    "                body = {\n",
    "                    'ride': {\n",
    "                        'PULocationID': row['PULocationID'],\n",
    "                        'DOLocationID': row['DOLocationID'],\n",
    "                        'trip_distance': row['trip_distance'],\n",
    "                    },\n",
    "                    'ride_id': row['ride_id']\n",
    "                } \n",
    "\n",
    "                prediction_event = predict_endpoint(body)\n",
    "                f_out.write(json.dumps(prediction_event) + '\\n')\n",
    "\n",
    "        today_midnight = tomorrow_midnight\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe6bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74778706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
